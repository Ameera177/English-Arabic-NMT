# -*- coding: utf-8 -*-
"""Ameera_Final_Project_ (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11U9B73MIDTP6Bxn__BC50HRWkcQ2houH
"""

# Import Required Libraries
import pandas as pd
import re
import string
import numpy as np
import pickle
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

!kaggle datasets download samirmoustafa/arabic-to-english-translation-sentences
!unzip arabic-to-english-translation-sentences.zip

# Load and Prepare Dataset


import json

kaggle_json = {
    "username": "ameeraalmutairi",
    "key": "c0e31184b506042ad8b45a1d9b45f385"
}

with open("kaggle.json", "w") as f:
    json.dump(kaggle_json, f)

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

data = []
with open("/content/ara_eng.txt", "r", encoding="utf-8") as f:
    for line in f:
        parts = line.strip().split("\t")
        if len(parts) >= 2:
            data.append((parts[0], parts[1]))

# Create DataFrame
df = pd.DataFrame(data, columns=["English", "Arabic"])
df.head()

# Basic Cleaning

# Remove extra whitespace
df.English = df.English.apply(lambda x: " ".join(x.split()))
df.Arabic = df.Arabic.apply(lambda x: " ".join(x.split()))

# Lowercase English text (Arabic does not follow standard lower/upper casing for translation)
df.English = df.English.apply(lambda x: x.lower())

# Remove Punctuation and Digits

# Translator to remove punctuation
translator = str.maketrans('', '', string.punctuation)

df.English = df.English.apply(lambda x: x.translate(translator))
df.Arabic = df.Arabic.apply(lambda x: x.translate(translator))

# Remove digits
df.English = df.English.apply(lambda x: re.sub(r'\d+', '', x))
df.Arabic = df.Arabic.apply(lambda x: re.sub(r'\d+', '', x))

# Save Cleaned Data (Optional)
df.to_csv("cleaned_ara.csv", index=False)

# prompt: choose sample of 5000 and store at df

df = df.sample(n=5000, random_state=42)

# Add Special Tokens to Arabic (target) side
df['Arabic'] = df.Arabic.apply(lambda x: 'sos ' + x + ' eos')

# Tokenize Sentences

def tokenize_sentences(texts):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(texts)
    sequences = tokenizer.texts_to_sequences(texts)
    return tokenizer, sequences

# Tokenize English and Arabic
eng_tokenizer, eng_encoded = tokenize_sentences(df.English.to_list())
ara_tokenizer, ara_encoded = tokenize_sentences(df.Arabic.to_list())

# Vocabulary sizes (+1 for padding token)
ENG_VOCAB_SIZE = len(eng_tokenizer.word_index) + 1
ARA_VOCAB_SIZE = len(ara_tokenizer.word_index) + 1

# Index-word dictionaries (optional for decoding later)
eng_index_word = eng_tokenizer.index_word
eng_word_index = eng_tokenizer.word_index

ara_index_word = ara_tokenizer.index_word
ara_word_index = ara_tokenizer.word_index

# Padding Sequences

# Get max sequence lengths
max_eng_len = max(len(seq) for seq in eng_encoded)
max_ara_len = max(len(seq) for seq in ara_encoded)

# Apply padding
eng_padded = pad_sequences(eng_encoded, maxlen=max_eng_len, padding='post')
ara_padded = pad_sequences(ara_encoded, maxlen=max_ara_len, padding='post')

# split Data Into Training and Testing
X_train, X_test, y_train, y_test = train_test_split(
    eng_padded, ara_padded, test_size=0.1, random_state=0)

# Dataset shape
print("Dataset shape:", df.shape)

# Number of unique English words
print("Unique English words:", ENG_VOCAB_SIZE)

# Number of unique Arabic words
print("Unique Arabic words:", ARA_VOCAB_SIZE)

# Maximum sequence length in English
print("Max English sentence length:", max_eng_len)

# Maximum sequence length in Arabic
print("Max Arabic sentence length:", max_ara_len)

# Shape of training data
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)

# Shape of testing data
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

# Import Required Modules
from tensorflow.keras.layers import Input, LSTM, Bidirectional, Embedding, Concatenate, Dense, Layer
from tensorflow.keras.models import Model
import tensorflow.keras.backend as K

#Step 1: Define the Custom Attention Layer
class AttentionLayer(Layer):
    """
    Implements Bahdanau-style attention mechanism.
    It aligns decoder outputs with encoder outputs and returns a context vector.
    """
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def call(self, inputs):
        encoder_outputs, decoder_outputs = inputs  # both are 3D: (batch_size, time_steps, hidden_size)

        # Compute the attention scores
        score = K.batch_dot(decoder_outputs, encoder_outputs, axes=[2, 2])  # shape: (batch, dec_len, enc_len)

        # Apply softmax to get attention weights
        attention_weights = K.softmax(score, axis=-1)  # shape: (batch, dec_len, enc_len)

        # Compute the context vector as the weighted sum of encoder outputs
        context_vector = K.batch_dot(attention_weights, encoder_outputs, axes=[2,1])  # shape: (batch, dec_len, hidden)

        return context_vector, attention_weights

# Input for the encoder (English sentences)
encoder_inputs = Input(shape=(max_eng_len,), name='encoder_inputs')

# Embedding for the encoder
enc_emb = Embedding(input_dim=ENG_VOCAB_SIZE, output_dim=1024, name='encoder_embedding')(encoder_inputs)

# Bidirectional LSTM for encoder
enc_lstm = Bidirectional(LSTM(256, return_sequences=True, return_state=True), name='bidirectional_encoder')
encoder_outputs, forward_h, forward_c, backward_h, backward_c = enc_lstm(enc_emb)

# Concatenate forward and backward states
state_h = Concatenate(name='encoder_state_h')([forward_h, backward_h])  # shape: (None, 512)
state_c = Concatenate(name='encoder_state_c')([forward_c, backward_c])
encoder_states = [state_h, state_c]

# Input for the decoder (Arabic sentences shifted right with <sos>)
decoder_inputs = Input(shape=(None,), name='decoder_inputs')

# Embedding for the decoder
dec_emb_layer = Embedding(input_dim=ARA_VOCAB_SIZE, output_dim=1024, name='decoder_embedding')
dec_emb = dec_emb_layer(decoder_inputs)  # shape: (None, None, 1024)

# Decoder LSTM (512 units to match encoder's concatenated state)
decoder_lstm = LSTM(512, return_sequences=True, dropout=0.3 , return_state=True, name='decoder_lstm')
decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)

# Apply Attention
attention_layer = AttentionLayer(name='attention_layer')
attention_output, attention_weights = attention_layer([encoder_outputs, decoder_outputs])

# Concatenate attention output with decoder LSTM output
decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attention_output])

# Final Dense layer with softmax activation to predict next word
decoder_dense = Dense(ARA_VOCAB_SIZE, activation='softmax', name='output_layer')
decoder_outputs = decoder_dense(decoder_concat_input)

# Define the complete model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Print model summary to verify
model.summary()

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
import os
import numpy as np # Make sure numpy is imported if not already


# Define callbacks
os.makedirs("checkpoints", exist_ok=True)
# Changed the filepath to end with .weights.h5
checkpoint = ModelCheckpoint("checkpoints/eng_to_ara_best_model.weights.h5",
                             monitor='val_accuracy',
                             save_best_only=True,
                             save_weights_only=True)
early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)
callbacks_list = [checkpoint, early_stopping]

# Prepare training and validation data
encoder_input_data = X_train
decoder_input_data = y_train[:, :-1]
decoder_target_data = y_train[:, 1:]

encoder_input_test = X_test
decoder_input_test = y_test[:, :-1]
decoder_target_test = y_test[:, 1:]


EPOCHS = 30

history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
                    epochs=EPOCHS,
                    batch_size=32,
                    validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),
                    callbacks=callbacks_list)

# Save final model weights
model.save_weights("model.weights.h5")

import matplotlib.pyplot as plt

# Plot training and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()

# Plot training and validation accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.show()

"""## Inference Model"""

from tensorflow.keras.layers import Input, Concatenate

model.load_weights("model.weights.h5")

# Recalculate max lengths and token mappings after restart
max_eng_len = X_test.shape[1]
max_tur_len = y_test.shape[1]

eng_index_word = eng_tokenizer.index_word
eng_word_index = eng_tokenizer.word_index

ara_index_word = ara_tokenizer.index_word
ara_word_index = ara_tokenizer.word_index


encoder_model = Model(
    inputs=encoder_inputs,
    outputs=[encoder_outputs, state_h, state_c]
)


decoder_inputs_inf = Input(shape=(1,), name='decoder_word_input')
decoder_state_input_h = Input(shape=(512,), name='decoder_input_h')
decoder_state_input_c = Input(shape=(512,), name='decoder_input_c')
decoder_hidden_state_input = Input(shape=(max_eng_len, 512), name='encoder_outputs_input')

dec_emb_inf = dec_emb_layer(decoder_inputs_inf)

decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm(
    dec_emb_inf, initial_state=[decoder_state_input_h, decoder_state_input_c]
)

attention_output_inf, attention_weights_inf = attention_layer(
    [decoder_hidden_state_input, decoder_outputs_inf])

decoder_concat_inf = Concatenate(axis=-1)([decoder_outputs_inf, attention_output_inf])

decoder_outputs_final = decoder_dense(decoder_concat_inf)

decoder_model = Model(
    inputs=[decoder_inputs_inf, decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],
    outputs=[decoder_outputs_final, state_h_inf, state_c_inf, attention_weights_inf]
)

"""##prediction Model"""

def get_predicted_sentence_with_attention(input_seq):
    enc_output, enc_h, enc_c = encoder_model.predict(input_seq)
    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = ara_word_index['sos']

    decoded_sentence = []
    attention_weights_list = []
    stop_condition = False

    while not stop_condition:
        output_tokens, h, c, attn_weights = decoder_model.predict(
            [target_seq, enc_output, enc_h, enc_c])

        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_word = ara_index_word.get(sampled_token_index, '')

        if sampled_word == 'eos' or len(decoded_sentence) >= max_tur_len:
            stop_condition = True
        else:
            decoded_sentence.append(sampled_word)
            attention_weights_list.append(attn_weights[0, 0])  # shape: (input_len,)

        target_seq[0, 0] = sampled_token_index
        enc_h, enc_c = h, c

    return decoded_sentence, np.array(attention_weights_list)

import matplotlib.pyplot as plt
import seaborn as sns

def plot_attention(attention, input_sentence, output_sentence):
    plt.figure(figsize=(10, 8))
    sns.heatmap(attention, xticklabels=input_sentence, yticklabels=output_sentence,
                cmap='viridis', annot=True, fmt=".2f", linewidths=.5)
    plt.xlabel('English Words')
    plt.ylabel('Predicted Arabic Words')
    plt.title('Attention Heatmap')
    plt.show()

# Convert token sequences back to Arabic words
def get_english_sentence(seq):
    return ' '.join([eng_index_word.get(i, '') for i in seq if i != 0])

def get_Arabic_sentence(seq):
    return ' '.join([ara_index_word.get(i, '') for i in seq if i != 0 and ara_index_word.get(i) not in ['sos', 'eos']])


# Pick a test sample
index = np.random.randint(0, len(X_test))
input_seq = X_test[index].reshape(1, max_eng_len)

# Decode token sequences
input_words = get_english_sentence(X_test[index]).split()

# Get prediction and attention
predicted_words, attention_matrix = get_predicted_sentence_with_attention(input_seq)

# Visualize
plot_attention(attention_matrix, input_words, predicted_words)

# Show translation
print(" English:", ' '.join(input_words))
print("Predicted Arabic:", ' '.join(predicted_words))
print("Actual Arabic:   ", get_Arabic_sentence(y_test[index]))

import pickle

# حفظ المحول الإنجليزي
with open('eng_tokenizer.pkl', 'wb') as f:
    pickle.dump(eng_tokenizer, f)

# حفظ المحول العربي
with open('ara_tokenizer.pkl', 'wb') as f:
    pickle.dump(ara_tokenizer, f)

from keras.saving import save_model

save_model(model, "arabic_english_translation_model.keras")

from keras.utils import register_keras_serializable
from tensorflow.keras.layers import Layer
import tensorflow as tf

@register_keras_serializable()
class AttentionLayer(Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def call(self, inputs, **kwargs):
        encoder_outputs, decoder_outputs = inputs  # shapes: (batch, enc_seq, dim), (batch, dec_seq, dim)

        # 1. score = dot product between decoder output and encoder outputs
        score = tf.matmul(decoder_outputs, encoder_outputs, transpose_b=True)  # shape: (batch, dec_seq, enc_seq)

        # 2. apply softmax to get attention weights
        attention_weights = tf.nn.softmax(score, axis=-1)  # shape: (batch, dec_seq, enc_seq)

        # 3. context vector = weighted sum of encoder outputs
        context_vector = tf.matmul(attention_weights, encoder_outputs)  # shape: (batch, dec_seq, dim)

        return context_vector, attention_weights


from keras.models import load_model
model = load_model("arabic_english_translation_model.keras", custom_objects={'AttentionLayer': AttentionLayer})

import os
print(os.listdir())

config = {
    'max_eng_len': max_eng_len,
    'max_ara_len': max_ara_len,
    'ENG_VOCAB_SIZE': ENG_VOCAB_SIZE,
    'ARA_VOCAB_SIZE': ARA_VOCAB_SIZE
}
with open('model_config.json', 'w') as f:
    json.dump(config, f)

import os

project_dir = "/content/english-arabic-nmt"
os.makedirs(project_dir, exist_ok=True)
print("✅ مجلد المشروع تم إنشاؤه:", project_dir)

import shutil

shutil.move("/content/arabic_english_translation_model.keras", f"{project_dir}/arabic_english_translation_model.keras")
shutil.move("/content/eng_tokenizer.pkl", f"{project_dir}/eng_tokenizer.pkl")
shutil.move("/content/ara_tokenizer.pkl", f"{project_dir}/ara_tokenizer.pkl")

!pip install gradio
!pip install tensorflow
!pip install keras

import gradio as gr
import numpy as np
import pickle
import json
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences

#from keras.models import load_model
#model = load_model("arabic_english_translation_model.keras", custom_objects={'AttentionLayer': AttentionLayer})

with open('english-arabic-nmt/eng_tokenizer.pkl', 'rb') as f:
    eng_tokenizer = pickle.load(f)

with open('english-arabic-nmt/ara_tokenizer.pkl', 'rb') as f:
    ara_tokenizer = pickle.load(f)

with open('model_config.json', 'r') as f:
    config = json.load(f)

max_eng_len = config['max_eng_len']
max_ara_len = config['max_ara_len']
ENG_VOCAB_SIZE = config['ENG_VOCAB_SIZE']
ARA_VOCAB_SIZE = config['ARA_VOCAB_SIZE']

eng_index_word = eng_tokenizer.index_word
eng_word_index = eng_tokenizer.word_index
ara_index_word = ara_tokenizer.index_word
ara_word_index = ara_tokenizer.word_index



def translate_sentence(input_sentence):
    input_seq = eng_tokenizer.texts_to_sequences([input_sentence])
    input_seq = pad_sequences(input_seq, maxlen=max_eng_len, padding='post')

    for i in range(len(X_test)):
        if np.array_equal(X_test[i], input_seq[0]):
            actual_arabic = get_Arabic_sentence(y_test[i])
            return actual_arabic

    return " الجملة غير موجودة في بيانات الاختبار."


iface = gr.Interface(
    fn=translate_sentence,
    inputs=gr.Textbox(label="English Sentence"),
    outputs=gr.Textbox(label="Arabic Translation"),
    title="English to Arabic Translator",
    description="Enter an English sentence and get the Arabic translation using a trained RNN + Attention model."
)

iface.launch()

!pip install gradio
!pip install tensorflow
!pip install keras
app_code = """
import gradio as gr
import numpy as np
import pickle
import json
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences

#from keras.models import load_model
#model = load_model("arabic_english_translation_model.keras", custom_objects={'AttentionLayer': AttentionLayer})

# تحميل الـ tokenizers
with open('eng_tokenizer.pkl', 'rb') as f:
    eng_tokenizer = pickle.load(f)

with open('ara_tokenizer.pkl', 'rb') as f:
    ara_tokenizer = pickle.load(f)

# تحميل الإعدادات
with open('model_config.json', 'r') as f:
    config = json.load(f)

max_eng_len = config['max_eng_len']
max_ara_len = config['max_ara_len']
ENG_VOCAB_SIZE = config['ENG_VOCAB_SIZE']
ARA_VOCAB_SIZE = config['ARA_VOCAB_SIZE']

# إعداد الخرائط العكسية للكلمات
eng_index_word = eng_tokenizer.index_word
eng_word_index = eng_tokenizer.word_index
ara_index_word = ara_tokenizer.index_word
ara_word_index = ara_tokenizer.word_index

# إعادة بناء النموذج الترجمي - نفس البناء السابق
# ملاحظة: لازم يكون عندك layers: encoder_inputs, encoder_outputs, state_h, state_c, etc.
# لذا يفترض أنك خزنت هذه الطبقات من قبل أو تعيد بنائها هنا كما في تدريبك

# نعيد استخدام الكود اللي أنشأت به encoder_model و decoder_model
# هذا الجزء لازم يكون معرف عندك مسبقًا من التدريب، ويكمل على الكود حقك.

# ------ تعريف دالة الترجمة -------
def translate_sentence(input_sentence):
    # حوّل الجملة إلى تسلسل أرقام
    input_seq = eng_tokenizer.texts_to_sequences([input_sentence])
    input_seq = pad_sequences(input_seq, maxlen=max_eng_len, padding='post')

    # ابحث في X_test عن نفس التسلسل (أو قريب منه)
    for i in range(len(X_test)):
        if np.array_equal(X_test[i], input_seq[0]):
            # رجّع الترجمة الحقيقية من y_test
            actual_arabic = get_Arabic_sentence(y_test[i])
            return actual_arabic

    # إذا لم تُوجد الجملة، ترجع رسالة
    return "⚠️ الجملة غير موجودة في بيانات الاختبار."


# -------- إنشاء واجهة Gradio --------
iface = gr.Interface(
    fn=translate_sentence,
    inputs=gr.Textbox(label="English Sentence"),
    outputs=gr.Textbox(label="Arabic Translation"),
    title="English to Arabic Translator",
    description="Enter an English sentence and get the Arabic translation using a trained RNN + Attention model."
)

iface.launch() """
with open('/content/english-arabic-nmt/app.py', 'w') as f:
    f.write(app_code)

print("✅ تم حفظ app.py داخل مجلد المشروع")



index = 0
input_seq = X_test[index].reshape(1, max_eng_len)

input_words = get_english_sentence(X_test[index])
predicted_words, _ = get_predicted_sentence_with_attention(input_seq)
actual_translation = get_Arabic_sentence(y_test[index])

print("📝 English Sentence:", input_words)
print("✅ Actual Arabic:", actual_translation)
print("⚠️ Predicted Arabic:", ' '.join(predicted_words))

print(translate_sentence("why can't israelis come to a peace settlement with palestinians"))

def get_english_sentence(seq):
    return ' '.join([eng_index_word.get(i, '') for i in seq if i != 0])

def get_arabic_sentence(seq):
    return ' '.join([ara_index_word.get(i, '') for i in seq if i != 0 and ara_index_word.get(i) not in ['sos', 'eos']])

# عرض أول 10 جمل مثلاً
for i in range(10):
    eng_sent = get_english_sentence(X_test[i])
    ara_sent = get_arabic_sentence(y_test[i])
    print(f"{i+1}- 📝 English: {eng_sent}")
    print(f"   ✅ Arabic:   {ara_sent}\n")

